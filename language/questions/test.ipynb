{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([[3, 2], [2], [5, 7, 8]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file=dict()\n",
    "file[\"asd\"]=[3,2]\n",
    "file[\"as\"]=[2]\n",
    "file[\"asq\"]=[5,7,8]\n",
    "import numpy as np\n",
    "n=2\n",
    "\n",
    "set(sum(file.values(), []))\n",
    "file.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "Usage: python questions.py corpus",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m Usage: python questions.py corpus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mert\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3406: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import sys\n",
    "import os\n",
    "import string\n",
    "import math\n",
    "\n",
    "FILE_MATCHES = 1\n",
    "SENTENCE_MATCHES = 1\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # Check command-line arguments\n",
    "    if len(sys.argv) != 2:\n",
    "        sys.exit(\"Usage: python questions.py corpus\")\n",
    "\n",
    "    # Calculate IDF values across files\n",
    "    files = load_files(sys.argv[1])\n",
    "    file_words = {\n",
    "        filename: tokenize(files[filename])\n",
    "        for filename in files\n",
    "    }\n",
    "    file_idfs = compute_idfs(file_words)\n",
    "\n",
    "    # Prompt user for query\n",
    "    query = set(tokenize(input(\"Query: \")))\n",
    "\n",
    "    # Determine top file matches according to TF-IDF\n",
    "    filenames = top_files(query, file_words, file_idfs, n=FILE_MATCHES)\n",
    "\n",
    "    # Extract sentences from top files\n",
    "    sentences = dict()\n",
    "    for filename in filenames:\n",
    "        for passage in files[filename].split(\"\\n\"):\n",
    "            for sentence in nltk.sent_tokenize(passage):\n",
    "                tokens = tokenize(sentence)\n",
    "                if tokens:\n",
    "                    sentences[sentence] = tokens\n",
    "\n",
    "    # Compute IDF values across sentences\n",
    "    idfs = compute_idfs(sentences)\n",
    "\n",
    "    # Determine top sentence matches\n",
    "    matches = top_sentences(query, sentences, idfs, n=SENTENCE_MATCHES)\n",
    "    for match in matches:\n",
    "        print(match)\n",
    "\n",
    "\n",
    "def load_files(directory):\n",
    "    \"\"\"\n",
    "    Given a directory name, return a dictionary mapping the filename of each\n",
    "    `.txt` file inside that directory to the file's contents as a string.\n",
    "    \"\"\"\n",
    "    dictionary = {}\n",
    "\n",
    "    for file in os.listdir(directory):\n",
    "        with open(os.path.join(directory, file), encoding=\"utf-8\") as ofile:\n",
    "            dictionary[file] = ofile.read()\n",
    "\n",
    "    return dictionary\n",
    "\n",
    "\n",
    "def tokenize(document):\n",
    "    \"\"\"\n",
    "    Given a document (represented as a string), return a list of all of the\n",
    "    words in that document, in order.\n",
    "    Process document by coverting all words to lowercase, and removing any\n",
    "    punctuation or English stopwords.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenized = nltk.tokenize.word_tokenize(document.lower())\n",
    "\n",
    "    final_list = [x for x in tokenized if x not in string.punctuation and x not in nltk.corpus.stopwords.words(\"english\")]\n",
    "\n",
    "    return final_list\n",
    "\n",
    "\n",
    "def compute_idfs(documents):\n",
    "    \"\"\"\n",
    "    Given a dictionary of `documents` that maps names of documents to a list\n",
    "    of words, return a dictionary that maps words to their IDF values.\n",
    "    Any word that appears in at least one of the documents should be in the\n",
    "    resulting dictionary.\n",
    "    \"\"\"\n",
    "    idf_dictio = {}\n",
    "    doc_len = len(documents)\n",
    "\n",
    "    unique_words = set(sum(documents.values(), []))\n",
    "\n",
    "    for word in unique_words:\n",
    "        count = 0\n",
    "        for doc in documents.values():\n",
    "            if word in doc:\n",
    "                count += 1\n",
    "\n",
    "        idf_dictio[word] = math.log(doc_len/count)\n",
    "\n",
    "    return idf_dictio\n",
    "\n",
    "\n",
    "def top_files(query, files, idfs, n):\n",
    "    \"\"\"\n",
    "    Given a `query` (a set of words), `files` (a dictionary mapping names of\n",
    "    files to a list of their words), and `idfs` (a dictionary mapping words\n",
    "    to their IDF values), return a list of the filenames of the the `n` top\n",
    "    files that match the query, ranked according to tf-idf.\n",
    "    \"\"\"\n",
    "    scores = {}\n",
    "    for filename, filecontent in files.items():\n",
    "        file_score = 0\n",
    "        for word in query:\n",
    "            if word in filecontent:\n",
    "                file_score += filecontent.count(word) * idfs[word]\n",
    "        if file_score != 0:\n",
    "            scores[filename] = file_score\n",
    "\n",
    "    sorted_by_score = [k for k, v in sorted(scores.items(), key=lambda x: x[1], reverse=True)]\n",
    "    return sorted_by_score[:n]\n",
    "\n",
    "\n",
    "def top_sentences(query, sentences, idfs, n):\n",
    "    \"\"\"\n",
    "    Given a `query` (a set of words), `sentences` (a dictionary mapping\n",
    "    sentences to a list of their words), and `idfs` (a dictionary mapping words\n",
    "    to their IDF values), return a list of the `n` top sentences that match\n",
    "    the query, ranked according to idf. If there are ties, preference should\n",
    "    be given to sentences that have a higher query term density.\n",
    "    \"\"\"\n",
    "    scores = {}\n",
    "    for sentence, sentwords in sentences.items():\n",
    "        score = 0\n",
    "        for word in query:\n",
    "            if word in sentwords:\n",
    "                score += idfs[word]\n",
    "\n",
    "        if score != 0:\n",
    "            density = sum([sentwords.count(x) for x in query]) / len(sentwords)\n",
    "            scores[sentence] = (score, density)\n",
    "\n",
    "    sorted_by_score = [k for k, v in sorted(scores.items(), key=lambda x: (x[1][0], x[1][1]), reverse=True)]\n",
    "\n",
    "    return sorted_by_score[:n]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "19f8f9ebccd493d1979261b88c51ecd06bf2efdee26e5f5e5ddb3d1c8ea2e26f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
